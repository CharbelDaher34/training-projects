{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 13:14:48 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PySpark analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/08 13:16:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/08 13:16:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/08 13:17:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/11/08 13:17:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved in analysis_results/pyspark\n",
      "Timing results saved in performance_results\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def log_timing(operation_name, start_time, timing_data):\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    timing_data.append(f\"{operation_name}: {duration:.2f} seconds\")\n",
    "    return end_time\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    # Create SparkSession with performance optimizations for local machine\n",
    "    # spark = (\n",
    "    #     SparkSession.builder.appName(\"Employee Analysis\")\n",
    "    #     # Memory configurations - carefully allocated for 16GB total RAM\n",
    "    #     .config(\"spark.driver.memory\", \"10g\")  # Leave ~6GB for OS and other processes\n",
    "    #     .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    #     .config(\"spark.memory.offHeap.size\", \"2g\")\n",
    "    #     # Execution configurations - optimized for 8 cores\n",
    "\n",
    "    #     # Serialization and compression\n",
    "    #     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    #     .config(\"spark.rdd.compress\", \"true\")\n",
    "    #     .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\n",
    "    #     # Local mode optimizatio        .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "ns\n",
    "    #     .config(\"spark.local.dir\", \"/tmp\")  # Ensure adequate temp space\n",
    "    #     .config(\"spark.sql.shuffle.partitions.local\", \"8\")\n",
    "    #     # Storage configurations\n",
    "    #     .config(\n",
    "    #         \"spark.sql.files.maxPartitionBytes\", \"64MB\"\n",
    "    #     )  # Smaller chunks for local processing\n",
    "    #     # Original config\n",
    "    #     .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    #     .master(\"local[*]\")  # Use all available cores\n",
    "    #     .getOrCreate()\n",
    "    # )\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ClusterStreamingApp\") \\\n",
    "        .master(\"spark://84.16.230.94:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.cores.max\", \"4\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .getOrCreate()\n",
    "    # Initialize timing data\n",
    "    timing_data = []\n",
    "    overall_start = time.time()\n",
    "\n",
    "    # Read CSV\n",
    "    print(\"Starting PySpark analysis...\")\n",
    "    start_time = time.time()\n",
    "    df = spark.read.csv(\"data/employee_records.csv\", header=True, inferSchema=True)\n",
    "    current_time = log_timing(\"Read CSV\", start_time, timing_data)\n",
    "\n",
    "    # Query 1: Average salary by department and experience level\n",
    "    start_time = current_time\n",
    "    dept_exp_salary = df.groupBy(\"department\", \"experience_level\").agg(\n",
    "        F.count(\"salary\").alias(\"count\"),\n",
    "        F.round(F.mean(\"salary\"), 2).alias(\"mean\"),\n",
    "        F.round(F.stddev(\"salary\"), 2).alias(\"std\"),\n",
    "        F.min(\"salary\").alias(\"min\"),\n",
    "        F.max(\"salary\").alias(\"max\"),\n",
    "    )\n",
    "    dept_exp_salary.cache()  # Cache for performance\n",
    "    dept_exp_salary.count()  # Force computation\n",
    "    current_time = log_timing(\n",
    "        \"Query 1: Department-Experience Level Salary Analysis\", start_time, timing_data\n",
    "    )\n",
    "\n",
    "    # Query 2: Employee retention analysis\n",
    "    start_time = current_time\n",
    "    retention_analysis = (\n",
    "        df.withColumn(\n",
    "            \"tenure_days\",\n",
    "            F.datediff(F.current_date(), F.to_date(\"join_date\", \"yyyy-MM-dd\")),\n",
    "        )\n",
    "        .groupBy(\"department\")\n",
    "        .agg(\n",
    "            F.round(F.mean(\"tenure_days\"), 2).alias(\"mean_tenure_days\"),\n",
    "            F.min(\"tenure_days\").alias(\"min_tenure_days\"),\n",
    "            F.max(\"tenure_days\").alias(\"max_tenure_days\"),\n",
    "            F.count(\"*\").alias(\"employee_count\"),\n",
    "            F.round(F.mean(\"salary\"), 2).alias(\"mean_salary\"),\n",
    "        )\n",
    "    )\n",
    "    retention_analysis.cache()\n",
    "    retention_analysis.count()\n",
    "    current_time = log_timing(\n",
    "        \"Query 2: Employee Retention Analysis\", start_time, timing_data\n",
    "    )\n",
    "\n",
    "    # Query 3: Complex performance metrics\n",
    "    start_time = current_time\n",
    "    performance_metrics = (\n",
    "        df.withColumn(\n",
    "            \"is_high_performer\",\n",
    "            F.when(\n",
    "                F.col(\"last_rating\").isin([\"Exceptional\", \"Exceeds Expectations\"]), 1\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .withColumn(\"salary_tile\", F.ntile(4).over(Window.orderBy(\"salary\")))\n",
    "        .groupBy(\"department\", \"salary_tile\")\n",
    "        .agg(\n",
    "            F.round(F.mean(\"is_high_performer\"), 3).alias(\"high_performer_ratio\"),\n",
    "            F.round(F.mean(\"projects_completed\"), 2).alias(\"avg_projects\"),\n",
    "            F.sum(\"projects_completed\").alias(\"total_projects\"),\n",
    "            F.round(\n",
    "                F.mean(F.when(F.col(\"remote_work_eligible\") == True, 1).otherwise(0)), 3\n",
    "            ).alias(\"remote_work_ratio\"),\n",
    "            F.round(\n",
    "                F.mean(F.when(F.col(\"bonus_eligible\") == True, 1).otherwise(0)), 3\n",
    "            ).alias(\"bonus_eligible_ratio\"),\n",
    "        )\n",
    "    )\n",
    "    performance_metrics.cache()\n",
    "    performance_metrics.count()\n",
    "    current_time = log_timing(\n",
    "        \"Query 3: Performance Metrics Analysis\", start_time, timing_data\n",
    "    )\n",
    "\n",
    "    # Query 4: Location and compensation analysis\n",
    "    start_time = current_time\n",
    "    location_comp = df.groupBy(\"country\", \"office\").agg(\n",
    "        F.round(F.mean(\"salary\"), 2).alias(\"mean_salary\"),\n",
    "        F.round(F.stddev(\"salary\"), 2).alias(\"std_salary\"),\n",
    "        F.count(\"*\").alias(\"employee_count\"),\n",
    "        F.round(F.mean(\"stock_options\"), 2).alias(\"mean_stock_options\"),\n",
    "        F.sum(\"stock_options\").alias(\"total_stock_options\"),\n",
    "        F.round(\n",
    "            F.mean(F.when(F.col(\"bonus_eligible\") == True, 1).otherwise(0)), 3\n",
    "        ).alias(\"bonus_eligible_ratio\"),\n",
    "    )\n",
    "    location_comp.cache()\n",
    "    location_comp.count()\n",
    "    current_time = log_timing(\n",
    "        \"Query 4: Location Compensation Analysis\", start_time, timing_data\n",
    "    )\n",
    "\n",
    "    # Query 5: Advanced filtering and window calculations\n",
    "    start_time = current_time\n",
    "    windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "    top_performers = (\n",
    "        df.withColumn(\"salary_rank\", F.dense_rank().over(windowSpec))\n",
    "        .filter(\n",
    "            (F.col(\"last_rating\").isin([\"Exceptional\", \"Exceeds Expectations\"]))\n",
    "            & (F.col(\"salary_rank\") <= 10)\n",
    "        )\n",
    "        .groupBy(\"department\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"count\"),\n",
    "            F.round(F.mean(\"salary\"), 2).alias(\"mean_salary\"),\n",
    "            F.max(\"salary\").alias(\"max_salary\"),\n",
    "            F.round(F.mean(\"projects_completed\"), 2).alias(\"mean_projects\"),\n",
    "            F.sum(\"projects_completed\").alias(\"total_projects\"),\n",
    "        )\n",
    "    )\n",
    "    top_performers.cache()\n",
    "    top_performers.count()\n",
    "    current_time = log_timing(\n",
    "        \"Query 5: Top Performers Analysis\", start_time, timing_data\n",
    "    )\n",
    "\n",
    "    # Calculate overall execution time\n",
    "    overall_duration = time.time() - overall_start\n",
    "    timing_data.append(f\"\\nTotal execution time: {overall_duration:.2f} seconds\")\n",
    "\n",
    "    # Save timing results\n",
    "    output_dir = Path(\"performance_results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    with open(output_dir / f\"pyspark_timing_{timestamp}.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(timing_data))\n",
    "\n",
    "    # Save analysis results\n",
    "    results_dir = Path(\"analysis_results/pyspark\")\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dept_exp_salary.toPandas().to_csv(results_dir / \"dept_exp_salary.csv\", index=False)\n",
    "    retention_analysis.toPandas().to_csv(\n",
    "        results_dir / \"retention_analysis.csv\", index=False\n",
    "    )\n",
    "    performance_metrics.toPandas().to_csv(\n",
    "        results_dir / \"performance_metrics.csv\", index=False\n",
    "    )\n",
    "    location_comp.toPandas().to_csv(results_dir / \"location_comp.csv\", index=False)\n",
    "    top_performers.toPandas().to_csv(results_dir / \"top_performers.csv\", index=False)\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    print(f\"Analysis complete. Results saved in {results_dir}\")\n",
    "    print(f\"Timing results saved in {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
