{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"),\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:17:18,154 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to localhost:9092 [('127.0.0.1', 9092) IPv4]\n",
      "2024-11-08 15:17:18,164 - INFO - Probing node bootstrap-0 broker version\n",
      "2024-11-08 15:17:18,168 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "2024-11-08 15:17:18,303 - INFO - Broker version identified as 2.5.0\n",
      "2024-11-08 15:17:18,306 - INFO - Set configuration api_version=(2, 5, 0) to skip auto check_version requests on startup\n",
      "2024-11-08 15:17:18,329 - INFO - Generating batch 1\n",
      "2024-11-08 15:17:20,671 - INFO - Sending batch 1 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:20,721 - INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: connecting to kafka:9092 [('127.0.0.1', 9092) IPv4]\n",
      "2024-11-08 15:17:20,725 - INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]>: Connection complete.\n",
      "2024-11-08 15:17:20,727 - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n",
      "2024-11-08 15:17:21,213 - INFO - Completed sending batch 1\n",
      "2024-11-08 15:17:21,218 - INFO - Generating batch 2\n",
      "2024-11-08 15:17:22,461 - INFO - Sending batch 2 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:22,819 - INFO - Completed sending batch 2\n",
      "2024-11-08 15:17:22,822 - INFO - Generating batch 3\n",
      "2024-11-08 15:17:24,033 - INFO - Sending batch 3 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:24,326 - INFO - Completed sending batch 3\n",
      "2024-11-08 15:17:24,329 - INFO - Generating batch 4\n",
      "2024-11-08 15:17:25,639 - INFO - Sending batch 4 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:26,258 - INFO - Completed sending batch 4\n",
      "2024-11-08 15:17:26,260 - INFO - Generating batch 5\n",
      "2024-11-08 15:17:27,374 - INFO - Sending batch 5 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:27,637 - INFO - Completed sending batch 5\n",
      "2024-11-08 15:17:27,639 - INFO - Generating batch 6\n",
      "2024-11-08 15:17:28,427 - INFO - Sending batch 6 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:28,786 - INFO - Completed sending batch 6\n",
      "2024-11-08 15:17:28,789 - INFO - Generating batch 7\n",
      "2024-11-08 15:17:30,069 - INFO - Sending batch 7 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:30,390 - INFO - Completed sending batch 7\n",
      "2024-11-08 15:17:30,392 - INFO - Generating batch 8\n",
      "2024-11-08 15:17:31,688 - INFO - Sending batch 8 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:32,179 - INFO - Completed sending batch 8\n",
      "2024-11-08 15:17:32,182 - INFO - Generating batch 9\n",
      "2024-11-08 15:17:33,880 - INFO - Sending batch 9 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:34,309 - INFO - Completed sending batch 9\n",
      "2024-11-08 15:17:34,312 - INFO - Generating batch 10\n",
      "2024-11-08 15:17:36,002 - INFO - Sending batch 10 with 1000 records to Kafka...\n",
      "2024-11-08 15:17:36,338 - INFO - Completed sending batch 10\n",
      "2024-11-08 15:17:36,341 - INFO - Generating batch 11\n",
      "2024-11-08 15:17:36,590 - INFO - Shutting down...\n",
      "2024-11-08 15:17:36,592 - INFO - Closing the Kafka producer with 9223372036.0 secs timeout.\n",
      "2024-11-08 15:17:36,596 - INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Closing connection. \n",
      "2024-11-08 15:17:36,603 - INFO - Producer closed. Total batches sent: 11\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 1000 # Number of records to generate in one batch\n",
    "KAFKA_TOPIC = 'testing'\n",
    "KAFKA_SERVER = 'localhost:9092'\n",
    "\n",
    "# Rich data schema configuration\n",
    "DEPARTMENTS = {\n",
    "    'Engineering': {'salary_range': (70000, 160000), 'positions': ['Software Engineer', 'DevOps Engineer', 'QA Engineer', 'Engineering Manager']},\n",
    "    'Sales': {'salary_range': (50000, 130000), 'positions': ['Sales Representative', 'Account Executive', 'Sales Manager', 'Sales Director']},\n",
    "    'Marketing': {'salary_range': (45000, 120000), 'positions': ['Marketing Specialist', 'Content Manager', 'Marketing Director', 'Brand Manager']},\n",
    "    'HR': {'salary_range': (40000, 110000), 'positions': ['HR Coordinator', 'HR Manager', 'Recruiter', 'HR Director']},\n",
    "    'Finance': {'salary_range': (55000, 140000), 'positions': ['Financial Analyst', 'Accountant', 'Finance Manager', 'Controller']}\n",
    "}\n",
    "\n",
    "OFFICE_LOCATIONS = {\n",
    "    'New York': {'timezone': 'America/New_York', 'country': 'USA'},\n",
    "    'San Francisco': {'timezone': 'America/Los_Angeles', 'country': 'USA'},\n",
    "    'London': {'timezone': 'Europe/London', 'country': 'UK'},\n",
    "    'Singapore': {'timezone': 'Asia/Singapore', 'country': 'Singapore'},\n",
    "    'Sydney': {'timezone': 'Australia/Sydney', 'country': 'Australia'}\n",
    "}\n",
    "\n",
    "EMPLOYMENT_STATUS = ['Full-time', 'Part-time', 'Contract', 'Remote']\n",
    "PERFORMANCE_RATINGS = ['Exceptional', 'Exceeds Expectations', 'Meets Expectations', 'Needs Improvement']\n",
    "\n",
    "def generate_record():\n",
    "    \"\"\"Generate a single employee record with detailed information\"\"\"\n",
    "    join_date = datetime.now() - timedelta(days=random.randint(0, 3650))\n",
    "    department = random.choice(list(DEPARTMENTS.keys()))\n",
    "    position = random.choice(DEPARTMENTS[department]['positions'])\n",
    "    office_location = random.choice(list(OFFICE_LOCATIONS.keys()))\n",
    "    \n",
    "    return {\n",
    "        'metadata': {\n",
    "            'record_id': str(uuid.uuid4()),\n",
    "            'timestamp': str(int(time.time() * 1000)),  # Convert to string to match schema\n",
    "            'version': '2.0',\n",
    "            'data_center': random.choice(['dc-east', 'dc-west', 'dc-eu']),\n",
    "        },\n",
    "        'employee': {\n",
    "            'id': f\"EMP-{str(uuid.uuid4())[:8].upper()}\",\n",
    "            'name': fake.name(),\n",
    "            'email': fake.email(),\n",
    "            'phone': fake.phone_number(),\n",
    "            'department': department,\n",
    "            'position': position,\n",
    "            'experience_level': random.choice(['Junior', 'Mid-level', 'Senior', 'Expert']),\n",
    "            'employment_status': random.choice(EMPLOYMENT_STATUS),\n",
    "        },\n",
    "        'compensation': {\n",
    "            'salary': random.randint(*DEPARTMENTS[department]['salary_range']),\n",
    "            'bonus_eligible': random.choice([True, False]),\n",
    "            'stock_options': random.randint(0, 10000) if random.random() > 0.5 else 0,\n",
    "        },\n",
    "        'location': {\n",
    "            'office': office_location,\n",
    "            'timezone': OFFICE_LOCATIONS[office_location]['timezone'],\n",
    "            'country': OFFICE_LOCATIONS[office_location]['country'],\n",
    "            'remote_work_eligible': random.choice([True, False]),\n",
    "        },\n",
    "        'dates': {\n",
    "            'join_date': join_date.strftime('%Y-%m-%d'),\n",
    "            'last_promotion_date': (join_date + timedelta(days=random.randint(180, 1000))).strftime('%Y-%m-%d'),\n",
    "            'last_review_date': (datetime.now() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d'),\n",
    "        },\n",
    "        'performance': {\n",
    "            'last_rating': random.choice(PERFORMANCE_RATINGS),\n",
    "            'rating_date': (datetime.now() - timedelta(days=random.randint(0, 180))).strftime('%Y-%m-%d'),\n",
    "            'projects_completed': random.randint(1, 20),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_SERVER,\n",
    "        value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    "    )\n",
    "\n",
    "    batch_count = 0\n",
    "    try:\n",
    "        while True:\n",
    "            batch_count += 1\n",
    "            logger.info(f\"Generating batch {batch_count}\")\n",
    "            \n",
    "            # Generate batch of records\n",
    "            records = []\n",
    "            for _ in range(BATCH_SIZE):\n",
    "                records.append(generate_record())\n",
    "            \n",
    "            # Create a wrapper object with metadata and records array\n",
    "            batch_data = {\n",
    "                \"batch_id\": str(uuid.uuid4()),\n",
    "                \"batch_timestamp\": int(time.time() * 1000),\n",
    "                \"record_count\": len(records),\n",
    "                \"records\": records\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Sending batch {batch_count} with {len(records)} records to Kafka...\")\n",
    "            producer.send(KAFKA_TOPIC, value=batch_data)\n",
    "            producer.flush()\n",
    "            logger.info(f\"Completed sending batch {batch_count}\")\n",
    "            \n",
    "            # Print sample of the data being sent\n",
    "            # logger.info(\"Sample of batch data structure:\")\n",
    "            # sample_json = json.dumps(batch_data, indent=2)\n",
    "            # logger.info(f\"\\n{sample_json}...\")  # Show first 1000 chars as sample\n",
    "            \n",
    "            time.sleep(0)  # Wait 5 seconds between batches\n",
    "            if(batch_count==100):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Shutting down...\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "        logger.info(f\"Producer closed. Total batches sent: {batch_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
