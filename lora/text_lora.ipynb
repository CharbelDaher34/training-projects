{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet transformers accelerate evaluate datasets peft torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use GPT-2 as our base model for text generation. We'll fine-tune it on two different datasets using LoRA to demonstrate efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2f} MB\")\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0\n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
    "    return dataset_splits.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Synthetic Datasets\n",
    "\n",
    "We'll create two synthetic datasets:\n",
    "1. A dataset of technical documentation-style text\n",
    "2. A dataset of creative story-style text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# Technical documentation dataset generator\n",
    "def generate_tech_docs(num_samples=100):\n",
    "    topics = [\"API\", \"Database\", \"Network\", \"Security\", \"Cloud\"]\n",
    "    verbs = [\"configure\", \"implement\", \"deploy\", \"optimize\", \"secure\"]\n",
    "    components = [\"server\", \"application\", \"system\", \"framework\", \"protocol\"]\n",
    "    \n",
    "    texts = []\n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(topics)\n",
    "        verb = random.choice(verbs)\n",
    "        component = random.choice(components)\n",
    "        text = f\"How to {verb} a {topic} {component}. First, ensure all prerequisites are met. \"\n",
    "        text += f\"Then, follow these steps to {verb} the {component}. \"\n",
    "        text += f\"This guide covers best practices for {topic} implementation.\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Creative story dataset generator\n",
    "def generate_stories(num_samples=100):\n",
    "    characters = [\"wizard\", \"knight\", \"dragon\", \"princess\", \"merchant\"]\n",
    "    actions = [\"journeyed\", \"discovered\", \"battled\", \"created\", \"explored\"]\n",
    "    places = [\"ancient castle\", \"mystical forest\", \"hidden cave\", \"magical realm\", \"forgotten city\"]\n",
    "    \n",
    "    texts = []\n",
    "    for _ in range(num_samples):\n",
    "        character = random.choice(characters)\n",
    "        action = random.choice(actions)\n",
    "        place = random.choice(places)\n",
    "        text = f\"The {character} {action} through the {place}. \"\n",
    "        text += f\"In this magical adventure, they encountered wonders beyond imagination. \"\n",
    "        text += f\"The story of the {character} became legendary throughout the land.\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Generate datasets\n",
    "dataset1 = generate_tech_docs(2)\n",
    "dataset2 = generate_stories(2)\n",
    "\n",
    "# Split datasets\n",
    "dataset1_train, dataset1_test = split_dataset(dataset1)\n",
    "dataset2_train, dataset2_test = split_dataset(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model1\": {\n",
    "        \"train_data\": dataset1_train,\n",
    "        \"test_data\": dataset1_test,\n",
    "        \"epochs\": 1,\n",
    "        \"path\": \"./lora-text-model1\",\n",
    "        \"description\": \"Technical documentation model\"\n",
    "    },\n",
    "    \"model2\": {\n",
    "        \"train_data\": dataset2_train,\n",
    "        \"test_data\": dataset2_test,\n",
    "        \"epochs\": 1,\n",
    "        \"path\": \"./lora-text-model2\",\n",
    "        \"description\": \"Creative story model\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Create labels for causal language modeling (next token prediction)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "    return model_inputs\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charbel/anaconda3/envs/defaultenv/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/charbel/anaconda3/envs/defaultenv/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Changed from [\"query\", \"value\"] to target GPT-2's attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # Added task type for causal language modeling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Technical documentation model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d54e4ec99014abaa856e76e3945311d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f6f5cde4b4007b04d2c9ed9b31355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "/home/charbel/anaconda3/envs/defaultenv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:711: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical documentation model trainable parameters: 1,622,016/126,061,824 (1.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charbel/anaconda3/envs/defaultenv/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharbeldaher34\u001b[0m (\u001b[33mcharbeldaher34-lebanese-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2187505be614e9c9eacdcf3e7febc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112668788887782, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/charbel/Desktop/Eurisko/training-projects/lora/wandb/run-20241115_125718-tabg5ofc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/charbeldaher34-lebanese-university/huggingface/runs/tabg5ofc' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/charbeldaher34-lebanese-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/charbeldaher34-lebanese-university/huggingface' target=\"_blank\">https://wandb.ai/charbeldaher34-lebanese-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/charbeldaher34-lebanese-university/huggingface/runs/tabg5ofc' target=\"_blank\">https://wandb.ai/charbeldaher34-lebanese-university/huggingface/runs/tabg5ofc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188adfcd78dc4b2da3e04f2de17e1582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3174bfef6644a0eaaa1934a965d96d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.811991691589355, 'eval_runtime': 0.2944, 'eval_samples_per_second': 3.397, 'eval_steps_per_second': 3.397, 'epoch': 1.0}\n",
      "{'train_runtime': 4.2998, 'train_samples_per_second': 0.233, 'train_steps_per_second': 0.233, 'train_loss': 7.141759395599365, 'epoch': 1.0}\n",
      "Model saved to ./lora-text-model1\n",
      "Model size: 6.50 MB\n",
      "\n",
      "Training Creative story model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20488d3d30184f1f8211d3e976fb3581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c68bda65ed4f9a8c10571e5889e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative story model trainable parameters: 1,622,016/126,061,824 (1.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charbel/anaconda3/envs/defaultenv/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11dec2bcd61c4ff082f6bcfb5822d66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8e832b93b84ad6a4d0c441fe6d9f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.170618057250977, 'eval_runtime': 0.3151, 'eval_samples_per_second': 3.173, 'eval_steps_per_second': 3.173, 'epoch': 1.0}\n",
      "{'train_runtime': 1.7788, 'train_samples_per_second': 0.562, 'train_steps_per_second': 0.562, 'train_loss': 7.800537109375, 'epoch': 1.0}\n",
      "Model saved to ./lora-text-model2\n",
      "Model size: 6.50 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "for cfg in config.values():\n",
    "    print(f\"\\nTraining {cfg['description']}\")\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_dataset = prepare_dataset(cfg['train_data'])\n",
    "    eval_dataset = prepare_dataset(cfg['test_data'])\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    print_trainable_parameters(peft_model, cfg['description'])\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_arguments,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    \n",
    "    # Train and save\n",
    "    trainer.train()\n",
    "    peft_model.save_pretrained(cfg['path'])\n",
    "    print(f\"Model saved to {cfg['path']}\")\n",
    "    print_model_size(cfg['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Technical documentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How to configure a secure API server\n",
      "Generated: How to configure a secure API server?\n",
      "\n",
      "There are two different ways for setting up a secure server; the simplest is by creating your own SSL certificate. The second method is to use a custom formatter.\n",
      "\n",
      "Let's say you want to access a database using SSL. You need to use the following code to use the formatter.\n",
      "\n",
      "<form action=\"form.bodyForm\"> <input type=\"text\" name=\"databaseName\" value=\"SOCIAL-PASSW\n",
      "\n",
      "\n",
      "Testing Creative story model\n",
      "Prompt: Once upon a time, a brave wizard\n",
      "Generated: Once upon a time, a brave wizard was able to turn back time and return the time she had been on to that which she had been given. The time she had been given was the same as her original time and she was able to return time to her original time.\n",
      "\n",
      "The Time of the Dragon has been named by Wizards of the Coast in a new adventure, The Time of the Dragon: The Quest for the Crown. The time she was given is the same as her original time and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Load and test both models\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "for cfg in config.values():\n",
    "    print(f\"\\nTesting {cfg['description']}\")\n",
    "    \n",
    "    # Load the LoRA model\n",
    "    model = PeftModel.from_pretrained(base_model, cfg['path'])\n",
    "    \n",
    "    # Generate some text\n",
    "    if \"Technical\" in cfg['description']:\n",
    "        prompt = \"How to configure a secure API server\"\n",
    "    else:\n",
    "        prompt = \"Once upon a time, a brave wizard\"\n",
    "        \n",
    "    generated_text = generate_text(model, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defaultenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
